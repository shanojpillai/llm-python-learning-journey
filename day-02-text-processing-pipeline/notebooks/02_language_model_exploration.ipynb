{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Streamlit application for text processing pipeline.\n",
    "\"\"\"\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the src directory to path\n",
    "sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "from src.preprocessing.cleaner import clean_text\n",
    "from src.preprocessing.tokenization import (\n",
    "    basic_word_tokenize,\n",
    "    advanced_tokenize,\n",
    "    visualize_tokenization_comparison\n",
    ")\n",
    "from src.vocabulary.vocab_builder import (\n",
    "    build_vocabulary,\n",
    "    tokenize_and_encode,\n",
    "    decode_token_ids\n",
    ")\n",
    "from src.models.embeddings import create_embeddings, visualize_embeddings\n",
    "from src.models.language_model import SimpleLanguageModel, generate_text\n",
    "from src.visualization.visualize import plot_token_distribution\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"Text Processing Pipeline\",\n",
    "    page_icon=\"ðŸ“š\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "st.title(\"Text Processing Pipeline for Language Models\")\n",
    "\n",
    "st.markdown(\"\"\"\n",
    "This application demonstrates a complete text processing pipeline for language models.\n",
    "\"\"\")\n",
    "\n",
    "# Sidebar for navigation\n",
    "st.sidebar.title(\"Navigation\")\n",
    "page = st.sidebar.radio(\n",
    "    \"Go to\",\n",
    "    [\"Introduction\", \"Tokenization\", \"Vocabulary Building\", \n",
    "     \"Token IDs & Special Tokens\", \"Word Embeddings\", \"Mini Language Model\"]\n",
    ")\n",
    "\n",
    "# Create a placeholder for sample data\n",
    "@st.cache_data\n",
    "def load_sample_data():\n",
    "    try:\n",
    "        # Try to load sample headlines\n",
    "        with open(\"data/raw/sample_headlines.txt\", \"r\") as f:\n",
    "            headlines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "    except:\n",
    "        # If file doesn't exist, use default sample data\n",
    "        headlines = [\n",
    "            \"Scientists discover new planet in nearby solar system\",\n",
    "            \"Local man finds $10,000 in old couch\",\n",
    "            \"World leaders agree on climate change action plan\",\n",
    "            \"Study shows coffee may help prevent certain diseases\",\n",
    "            \"Tech company unveils revolutionary new smartphone\",\n",
    "            \"Stock market reaches all-time high amid economic recovery\"\n",
    "        ]\n",
    "    \n",
    "    return headlines\n",
    "\n",
    "sample_data = load_sample_data()\n",
    "\n",
    "# Introduction page\n",
    "if page == \"Introduction\":\n",
    "    st.header(\"Introduction to Text Processing for LLMs\")\n",
    "    \n",
    "    st.markdown(\"\"\"\n",
    "    ### Key Concepts We'll Explore:\n",
    "    \n",
    "    1. **Tokenization**: Breaking down text into smaller units (words, subwords, characters)\n",
    "    2. **Vocabulary Building**: Creating a list of unique tokens from our text\n",
    "    3. **Converting Tokens to IDs**: Transforming tokens into numerical representations\n",
    "    4. **Special Context Tokens**: Adding special tokens to provide context\n",
    "    5. **Word Embeddings**: Creating vector representations of words\n",
    "    6. **Simple Language Model**: Building a basic model that generates text\n",
    "    \n",
    "    This interactive application will guide you through each of these concepts with practical examples.\n",
    "    \"\"\")\n",
    "\n",
    "# Tokenization page\n",
    "elif page == \"Tokenization\":\n",
    "    st.header(\"Tokenization\")\n",
    "    \n",
    "    st.markdown(\"\"\"\n",
    "    **Tokenization** is the process of breaking text into smaller units called tokens.\n",
    "    These tokens can be words, subwords, or characters. The choice of tokenization\n",
    "    strategy significantly impacts the vocabulary size and model performance.\n",
    "    \n",
    "    Let's explore different tokenization approaches:\n",
    "    \"\"\")\n",
    "    \n",
    "    # Sample text for tokenization demo\n",
    "    sample_text = st.text_area(\n",
    "        \"Enter some text to tokenize\",\n",
    "        \"Hello, world! This is a test. Can tokenization handle punctuation and special-characters?\",\n",
    "        height=100\n",
    "    )\n",
    "    \n",
    "    st.subheader(\"1. Basic Word Tokenization\")\n",
    "    st.markdown(\"\"\"\n",
    "    Word-level tokenization splits text by whitespace and treats punctuation as part of words.\n",
    "    This is simple but can lead to a large vocabulary and doesn't handle morphological variations well.\n",
    "    \"\"\")\n",
    "    \n",
    "    word_tokens = basic_word_tokenize(sample_text)\n",
    "    st.write(f\"Word Tokens ({len(word_tokens)} tokens):\", word_tokens)\n",
    "    \n",
    "    st.subheader(\"2. Advanced Tokenization\")\n",
    "    st.markdown(\"\"\"\n",
    "    This approach handles special characters, punctuation, and whitespace as separate tokens.\n",
    "    It provides more granular control over the tokenization process.\n",
    "    \"\"\")\n",
    "    \n",
    "    advanced_tokens = advanced_tokenize(sample_text)\n",
    "    st.write(f\"Advanced Tokens ({len(advanced_tokens)} tokens):\", advanced_tokens)\n",
    "    \n",
    "    # Tokenization comparison chart\n",
    "    st.subheader(\"Tokenization Comparison\")\n",
    "    fig = visualize_tokenization_comparison(sample_text)\n",
    "    st.pyplot(fig)\n",
    "    \n",
    "    # Add the remaining pages for vocabulary building, token IDs, word embeddings, and language model\n",
    "    # These can be implemented as you progress through the project\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This will only run when the script is executed directly\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
