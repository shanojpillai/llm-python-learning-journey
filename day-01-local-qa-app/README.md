# Day 1: Building a Local LLM Q&A Assistant

This project involves building a Q&A application that leverages Ollama to run open-source Large Language Models (LLMs) locally on a CPU. The application features a Streamlit-based web interface and follows software engineering best practices.

## Setup Instructions

### Prerequisites
- Python 3.8+
- Git
- Ollama installed and running (https://ollama.ai/)

### Installation
1. Create and activate a virtual environment:
   ```bash
   python -m venv venv
   # On Windows:
   venv\Scripts\activate
   # On macOS/Linux:
   source venv/bin/activate